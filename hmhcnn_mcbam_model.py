# -*- coding: utf-8 -*-
"""HMHCNN-mCBAM)_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/131SDRNfqnTkoeupVx6JGTf-TNzu7g89K
"""

# === Multi-Head CNN with CBAM + Hierarchical Convolution ===
def multihead_cnn_with_attention_and_cbam():
    input_layer = Input(shape=(256, 256, 3))

    # Multi-head CNN Backbone
    x1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x1 = MaxPooling2D((2, 2))(x1)

    x2 = Conv2D(48, (3, 3), activation='relu', padding='same')(x1)
    x2 = MaxPooling2D((2, 2))(x2)

    x3 = Conv2D(64, (5, 5), activation='relu', padding='same')(x2)
    x3 = MaxPooling2D((2, 2))(x3)
    x3 = Dropout(0.02)(x3)

    x4 = Conv2D(32, (3, 3), activation='relu', padding='same')(x3)
    x4 = MaxPooling2D((2, 2))(x4)
    x4 = BatchNormalization()(x4)
    x4 = Dropout(0.02)(x4)

    # Hierarchical Convolution
    hierarchical_conv = HierarchicalConvolution(filters=32, kernel_sizes=[(3, 3), (5, 5), (7, 7)])(x4)

    # CBAM Attention
    cbam_output = cbam_block(hierarchical_conv)

    # Fully Connected Layers
    flattened_output = Flatten()(cbam_output)
    z = Dense(56, activation='relu')(flattened_output)
    z = BatchNormalization()(z)

    z = Dense(48, activation='relu')(z)
    z = BatchNormalization()(z)

    z = Dense(32, activation='relu')(z)
    z = BatchNormalization()(z)

    output_layer = Dense(4, activation='softmax')(z)

    model = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])
    model.summary()

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model