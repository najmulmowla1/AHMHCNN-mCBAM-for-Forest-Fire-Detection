# -*- coding: utf-8 -*-
"""MCBAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4n6hw0z7k0HrA7ntpBd5GPCupVPhrvO
"""

#Modified CBAM

from tensorflow.keras import backend as K
from tensorflow.keras.initializers import GlorotUniform
from tensorflow.keras.layers import LayerNormalization

def channel_attention(input_feature, ratio=8):
    channel_dim = input_feature.shape[-1]

    shared_layer_one = Dense(channel_dim // ratio, activation='relu', kernel_initializer=GlorotUniform(), use_bias=True, bias_initializer='zeros')
    shared_layer_two = Dense(channel_dim, kernel_initializer=GlorotUniform(), use_bias=True, bias_initializer='zeros')

    avg_pool = GlobalAveragePooling2D()(input_feature)
    avg_pool = Reshape((1, 1, channel_dim))(avg_pool)
    avg_pool = shared_layer_one(avg_pool)
    avg_pool = shared_layer_two(avg_pool)

    max_pool = GlobalMaxPooling2D()(input_feature)
    max_pool = Reshape((1, 1, channel_dim))(max_pool)
    max_pool = shared_layer_one(max_pool)
    max_pool = shared_layer_two(max_pool)

    attention = Add()([avg_pool, max_pool])
    attention = Activation('relu')(attention)

    return Multiply()([input_feature, attention])

def spatial_attention(input_feature):
    kernel_size = 7

    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(input_feature)
    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(input_feature)
    concat = Concatenate(axis=3)([avg_pool, max_pool])

    # Adding layer normalization
    concat = LayerNormalization()(concat)

    feature = Conv2D(filters=1, kernel_size=kernel_size, strides=1, padding='same', activation='relu', kernel_initializer=GlorotUniform(), use_bias=False)(concat)

    # Attention gating
    gating = Conv2D(filters=1, kernel_size=1, strides=1, padding='same', activation='sigmoid', kernel_initializer=GlorotUniform(), use_bias=False)(concat)
    gated_feature = Multiply()([feature, gating])

    return Multiply()([input_feature, gated_feature])

def cbam_block(input_feature):
    channel = channel_attention(input_feature)
    spatial = spatial_attention(channel)

    return spatial